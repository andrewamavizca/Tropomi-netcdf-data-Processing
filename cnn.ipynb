{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Automated Detection of Super Emitters Using Sattelite Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 13:05:55.507253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-04 13:05:55.515363: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-04 13:05:55.517638: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-04 13:05:55.523598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-04 13:05:56.106753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import h5py\n",
    "import cv2\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import os\n",
    "import warnings\n",
    "import absl.logging\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Suppress Abseil logs\n",
    "absl.logging.set_verbosity('error')\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optionally suppress specific warnings, like the lambda serialization warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"The object being serialized includes a `lambda`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom binary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable(name=\"WeightedBinaryCrossentropy\")\n",
    "def weighted_binary_crossentropy(y_true, y_pred, weight_0=1.0, weight_1=2.0):\n",
    "    # Calculate the standard binary cross-entropy loss\n",
    "    bce = BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    loss = bce(y_true, y_pred)\n",
    "\n",
    "    # Apply weights: weight_1 for positive class (y_true=1) and weight_0 for negative class (y_true=0)\n",
    "    weight_vector = y_true * weight_1 + (1.0 - y_true) * weight_0\n",
    "    weighted_loss = loss * weight_vector\n",
    "\n",
    "    # Return the mean loss\n",
    "    return tf.reduce_mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a named loss function\n",
    "@register_keras_serializable()\n",
    "def custom_weighted_binary_crossentropy(y_true, y_pred):\n",
    "    return weighted_binary_crossentropy(y_true, y_pred, weight_0=1.0, weight_1=2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = [\n",
    "            ('xch4_corrected', 'f4', ()),\n",
    "            ('latitude_corners', 'f4', (4,)),\n",
    "            ('longitude_corners', 'f4', (4,)),\n",
    "            ('u10', 'f4', ()),\n",
    "            ('v10', 'f4', ()),\n",
    "            ('latitude_center', 'f4', ()),\n",
    "            ('longitude_center', 'f4', ()),\n",
    "            ('scanline', 'i4', ()),\n",
    "            ('ground_pixel', 'i4', ()),\n",
    "            ('time', 'i4', (7,)),\n",
    "            ('solar_zenith_angle', 'f4', ()),\n",
    "            ('viewing_zenith_angle', 'f4', ()),\n",
    "            ('relative_azimuth_angle', 'f4', ()),\n",
    "            ('altitude_levels', 'f4', (13,)),\n",
    "            ('surface_altitude', 'f4', ()),\n",
    "            ('surface_altitude_stdv', 'f4', ()),\n",
    "            ('dp', 'f4', ()),\n",
    "            ('surface_pressure', 'f4', ()),\n",
    "            ('dry_air_subcolumns', 'f4', (12,)),\n",
    "            ('fluorescence_apriori', 'f4', ()),\n",
    "            ('cloud_fraction', 'f4', (4,)),\n",
    "            ('weak_h2o_column', 'f4', ()),\n",
    "            ('strong_h2o_column', 'f4', ()),\n",
    "            ('weak_ch4_column', 'f4', ()),\n",
    "            ('strong_ch4_column', 'f4', ()),\n",
    "            ('cirrus_reflectance', 'f4', ()),\n",
    "            ('stdv_h2o_ratio', 'f4', ()),\n",
    "            ('stdv_ch4_ratio', 'f4', ()),\n",
    "            ('xch4', 'f4', ()),\n",
    "            ('xch4_precision', 'f4', ()),\n",
    "            ('xch4_column_averaging_kernel', 'f4', (12,)),\n",
    "            ('ch4_profile_apriori', 'f4', (12,)),\n",
    "            ('xch4_apriori', 'f4', ()),\n",
    "            ('fluorescence', 'f4', ()),\n",
    "            ('co_column', 'f4', ()),\n",
    "            ('co_column_precision', 'f4', ()),\n",
    "            ('h2o_column', 'f4', ()),\n",
    "            ('h2o_column_precision', 'f4', ()),\n",
    "            ('spectral_shift', 'f4', (2,)),\n",
    "            ('aerosol_size', 'f4', ()),\n",
    "            ('aerosol_size_precision', 'f4', ()),\n",
    "            ('aerosol_column', 'f4', ()),\n",
    "            ('aerosol_column_precision', 'f4', ()),\n",
    "            ('aerosol_altitude', 'f4', ()),\n",
    "            ('aerosol_altitude_precision', 'f4', ()),\n",
    "            ('aerosol_optical_thickness', 'f4', (2,)),\n",
    "            ('surface_albedo', 'f4', (2,)),\n",
    "            ('surface_albedo_precision', 'f4', (2,)),\n",
    "            ('reflectance_max', 'f4', (2,)),\n",
    "            ('convergence', 'i4', ()),\n",
    "            ('iterations', 'i4', ()),\n",
    "            ('chi_squared', 'f4', ()),\n",
    "            ('chi_squared_band', 'f4', (2,)),\n",
    "            ('number_of_spectral_points_in_retrieval', 'i4', (2,)),\n",
    "            ('degrees_of_freedom', 'f4', ()),\n",
    "            ('degrees_of_freedom_ch4', 'f4', ()),\n",
    "            ('degrees_of_freedom_aerosol', 'f4', ()),\n",
    "            ('signal_to_noise_ratio', 'f4', (2,)),\n",
    "            ('rms', 'f4', ())\n",
    "        ]\n",
    "\n",
    "channel_map = {}\n",
    "current_channel = 0\n",
    "\n",
    "for name, field_type, *field_shape in data_type:\n",
    "    # Ensure current_channel is an integer\n",
    "    current_channel = int(current_channel)\n",
    "\n",
    "    if not field_shape:  # Scalar field\n",
    "        channel_map[name] = slice(current_channel, current_channel + 1)\n",
    "        current_channel += 1\n",
    "    else:  # Multi-dimensional field\n",
    "        total_channels = int(np.prod(field_shape))\n",
    "        channel_map[name] = slice(current_channel, current_channel + total_channels)\n",
    "        current_channel += total_channels\n",
    "\n",
    "# Add the normalized methane variable as the last channel\n",
    "channel_map['normalized_ch4'] = slice(current_channel, current_channel + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size=32, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)  # Properly calling the superclass initializer\n",
    "        self.x_set = x_set\n",
    "        self.y_set = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.x_set))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.x_set) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index == self.__len__() - 1:  # Last batch\n",
    "            batch_indices = self.indices[index * self.batch_size:]  # Get all remaining indices\n",
    "        else:\n",
    "            batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        x_batch = self.x_set[batch_indices]\n",
    "        y_batch = self.y_set[batch_indices]\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle the indices after each epoch if shuffle is True\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_test = np.load(\"data/test/xtest.npy\", allow_pickle=True)[:, :, :, channel_map['normalized_ch4']]\n",
    "y_test = np.load(\"data/test/ytest.npy\", allow_pickle=True)\n",
    "\n",
    "x1_val = np.load(\"data/validation/xval.npy\", allow_pickle=True)[:, :, :, channel_map['normalized_ch4']]\n",
    "y_val = np.load(\"data/validation/yval.npy\", allow_pickle=True)\n",
    "\n",
    "x1_train = np.load(\"data/training/xtrain.npy\", allow_pickle=True)[:, :, :, channel_map['normalized_ch4']]\n",
    "y_train = np.load(\"data/training/ytrain.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(32, 32, 1)),\n",
    "    tf.keras.layers.Conv2D(96, (3, 3), activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.005)),\n",
    "    tf.keras.layers.Conv2D(96, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4096, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.005)),\n",
    "    tf.keras.layers.Dropout(0.4),  # 40% dropout\n",
    "    tf.keras.layers.Dense(70, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile your model with binary focal loss\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=custom_weighted_binary_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(y):\n",
    "    num_positives = np.sum(y == 1)\n",
    "    num_negatives = np.sum(y == 0)\n",
    "    class_weights = {0: 1.0, 1: 1.0*(num_negatives / num_positives) }\n",
    "    print(class_weights)\n",
    "    return class_weights\n",
    "\n",
    "# Set class weights: weight for positive class is double the inverse ratio\n",
    "class_weights = get_class_weights(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128\n",
    "train_generator = DataGenerator(x1_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "val_generator = DataGenerator(x1_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "test_generator = DataGenerator(x1_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set up tf.callbacks: early stopping and model checkpoint\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=16, restore_best_weights=True)\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Reduce learning rate with a patience of 5 epochs\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "# Combine the callbacks\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "# Assign the bias of the last layer (if needed)\n",
    "#model.layers[-1].bias.assign([-0.99619189])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights,\n",
    "    epochs=100,  # Training for a maximum of 100 epochs\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Suppress Abseil logs\n",
    "absl.logging.set_verbosity('error')\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optionally suppress specific warnings, like the lambda serialization warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"The object being serialized includes a `lambda`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
    "\n",
    "# Predict using the test generator\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert probabilities to class labels (assuming binary classification with a threshold of 0.5)\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "#Compute cohens kappa\n",
    "kappa = cohen_kappa_score(y_test, predicted_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Extracting TP, TN, FP, FN\n",
    "TP = conf_matrix[1, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Save the model with kapp, accuracy, precision, recall, and F1 score om the filename\n",
    "model.save(f\"models/k_{kappa:.4f}_TA_{acc:.4f}_P_{precision:.4f}_R_{recall:.4f}_F_{f1:.4f}.keras\")\n",
    "\n",
    "print(f\"k:{kappa} Test accuracy: {acc:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, F1 score: {f1:.4f}\")\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Plot model accuracy\n",
    "ax[0].plot(history.history['accuracy'], label='Train')\n",
    "ax[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(loc='lower right')\n",
    "\n",
    "# Plot model loss\n",
    "ax[1].plot(history.history['loss'], label='Train')\n",
    "ax[1].plot(history.history['val_loss'], label='Validation')\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Plume', 'Plume'], \n",
    "            yticklabels=['No Plume', 'Plume'], ax=ax[2])\n",
    "ax[2].set_xlabel('Predicted Label')\n",
    "ax[2].set_ylabel('True Label')\n",
    "ax[2].set_title(f'CNN Confusion Matrix\\nKappa: {kappa:.4f}  Accuracy: {acc:.4f}\\nPrecision: {precision:.4f}  Recall: {recall:.4f}  F1: {f1:.4f}', fontsize=10)\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots with CAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate your model using the Functional API\n",
    "inputs = tf.keras.Input(shape=(32, 32, 1))\n",
    "\n",
    "# Define the tf.keras.layers in your model\n",
    "x = tf.keras.layers.Conv2D(96, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = tf.keras.layers.Conv2D(96, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "conv_output = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='last_conv')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(conv_output)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "x = tf.keras.layers.Dense(70, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_heatmap(cam_model, input_image, scale_positive_factor=1.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = cam_model(input_image)\n",
    "        predicted_label = predictions.numpy()[0, 0]  # Predicted label for the current image\n",
    "\n",
    "        # Scale the loss for positive predictions\n",
    "        loss = predictions[:, 0]\n",
    "        if predicted_label > 0.5:  # Assuming 0.5 as the threshold for positive\n",
    "            loss *= scale_positive_factor\n",
    "\n",
    "    # Compute the gradients\n",
    "    gradients = tape.gradient(loss, conv_outputs)\n",
    "\n",
    "    # Compute the guided gradients\n",
    "    pooled_grads = tf.reduce_mean(gradients, axis=(0, 1, 2))\n",
    "\n",
    "    # Multiply each channel by the pooled gradient\n",
    "    conv_outputs = conv_outputs.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        conv_outputs[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # Compute the heatmap\n",
    "    heatmap = np.mean(conv_outputs, axis=-1)\n",
    "\n",
    "    # Apply ReLU to ensure only positive activations are considered\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "    # Normalize the heatmap between 0 and 1 for visualization\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    # Upscale the heatmap to match the input image size using bilinear interpolation\n",
    "    heatmap = cv2.resize(heatmap, (input_image.shape[2], input_image.shape[1]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Normalize the heatmap to ensure values are between 0 and 1\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= heatmap.max()\n",
    "\n",
    "    return heatmap, predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_model = tf.keras.models.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=[model.get_layer('last_conv').output, model.output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of images you want to visualize\n",
    "start_index = 500\n",
    "end_index = 600 # Adjust this if you want to limit the number of images\n",
    "\n",
    "# Loop through the range and only process images with a true label of 1 (Plume)\n",
    "for image_index in range(start_index, end_index):\n",
    "    true_label = y_test[image_index]  # True label for the current image\n",
    "    if true_label == 1:  # Only process images with a plume\n",
    "        true_label_desc = \"Plume\"\n",
    "\n",
    "        # Prepare the image to be passed to the model\n",
    "        input_image = x1_test[image_index:image_index + 1]\n",
    "\n",
    "        heatmap, predicted_label = get_heatmap(cam_model, input_image)\n",
    "\n",
    "        # Display the original image and the heatmap\n",
    "        plt.figure(figsize=(10, 5), dpi=50)\n",
    "\n",
    "        # Display the input image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image[0, :, :, 0], cmap='rainbow')\n",
    "        plt.title(f'True Label: {true_label_desc}')\n",
    "\n",
    "        # Display the Grad-CAM heatmap overlayed on the input image\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(heatmap, cmap='viridis', alpha=1)  # Overlay the heatmap on the image\n",
    "        plt.title(f'True Label: {true_label_desc}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Recreation for CAM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the named function\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading In 2020 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5ChannelDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, hdf5_file, dataset_name, channel_slice, batch_size=32, shuffle=False, **kwargs):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.dataset_name = dataset_name\n",
    "        self.channel_slice = channel_slice\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        with h5py.File(self.hdf5_file, 'r') as hf:\n",
    "            self.num_samples = hf[self.dataset_name].shape[0]\n",
    "            self.num_variables = hf[self.dataset_name].shape[1]  # Variables dimension\n",
    "\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        self.on_epoch_end()\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.floor(self.num_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        with h5py.File(self.hdf5_file, 'r') as hf:\n",
    "            # Load the batch data in shape (batch_size, variables, 32, 32)\n",
    "            batch_data = hf[self.dataset_name][batch_indices, :, :, :]\n",
    "\n",
    "        # Reshape the data to (batch_size, 32, 32, variables)\n",
    "        batch_data = np.transpose(batch_data, (0, 2, 3, 1))\n",
    "\n",
    "        # Slice the specific channel\n",
    "        batch_data = batch_data[:, :, :, self.channel_slice]\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indices after each epoch\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Model on 2020 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/k_0.9772_TA_0.9910_P_0.9871_R_0.9796_F_0.9833.keras')\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "# Create the test data generator\n",
    "test_generator2 = HDF5ChannelDataGenerator('2020/all_scenes_combined.h5', 'all_scenes', channel_map['normalized_ch4'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Generate predictions using the model\n",
    "predictions_2020 = model.predict(test_generator2)\n",
    "\n",
    "# Optionally, print or analyze the predictions\n",
    "print(\"Predictions:\", predictions_2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the number of plumes you want to process\n",
    "num_plumes_to_process = 50\n",
    "\n",
    "#pd2020[pd2020 <= 0.5] = 0\n",
    "# Get indices of all plumes\n",
    "plume_indices = [i for i in range(len(predictions_2020)) if predictions_2020[i] > 0.5]\n",
    "\n",
    "# Randomly sample indices from the plume_indices list\n",
    "random_plume_indices = random.sample(plume_indices, num_plumes_to_process)\n",
    "\n",
    "with h5py.File('2020/all_scenes_combined.h5', 'r') as hf:\n",
    "    for i in random_plume_indices:\n",
    "        # Extract the image based on the prediction index\n",
    "        input_image = hf['all_scenes'][i:i+1, :, :, :]  # Extract the full image\n",
    "        input_image = np.transpose(input_image, (0, 2, 3, 1))  # Reshape to (batch, 32, 32, variables)\n",
    "        input_image = input_image[:, :, :, channel_map['normalized_ch4']]  # Slice to get the required channel\n",
    "        \n",
    "        heatmap, predicted_label = get_heatmap(cam_model, input_image)\n",
    "\n",
    "        # Display the original image and the heatmap\n",
    "        plt.figure(figsize=(10, 5), dpi=50)\n",
    "\n",
    "        # Display the input image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_image[0, :, :, 0], cmap='rainbow')\n",
    "        plt.title(f'Prediction: {predictions_2020[i]}')\n",
    "\n",
    "        # Display the Grad-CAM heatmap overlayed on the input image\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(heatmap, cmap='viridis', alpha=1)  # Overlay the heatmap on the image\n",
    "        plt.title(f\"CAM visualization\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "# Function to compute plume shape and wind direction alignment\n",
    "def compute_plume_shape_and_direction(plume_mask, wind_x, wind_y):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(plume_mask)\n",
    "    wind_x = np.nanmean(wind_x)\n",
    "    wind_y = np.nanmean(wind_y)\n",
    "    \n",
    "    elongation_ratio = pca.explained_variance_ratio_[0] / pca.explained_variance_ratio_[1]\n",
    "\n",
    "    plume_direction = np.arctan2(pca.components_[0][1], pca.components_[0][0])\n",
    "    wind_direction = np.arctan2(wind_y, wind_x)\n",
    "\n",
    "    direction_difference = np.abs(wind_direction - plume_direction)\n",
    "    return elongation_ratio, direction_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute correlations\n",
    "def compute_correlation(xch4_values, param_values, mask):\n",
    "    return pearsonr(xch4_values[mask], param_values[mask])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_plume_source(mask, wind_u, wind_v):\n",
    "    indices = np.where(mask == True)  # Get indices where mask is high-confidence\n",
    "    min_dist = np.inf\n",
    "    source_idx = None\n",
    "    wind_u[np.isnan(wind_u)] = 0\n",
    "    wind_v[np.isnan(wind_v)] = 0\n",
    "    \n",
    "    # Iterate over all high-confidence pixels\n",
    "    for idx in zip(indices[0], indices[1]):\n",
    "        current_pos = np.array(idx)\n",
    "        wind_vector = -np.array([wind_u[idx], wind_v[idx]])  # Correctly reversed wind direction\n",
    "        potential_source = current_pos + wind_vector\n",
    "        \n",
    "        # Calculate distance to the nearest edge in the direction of wind\n",
    "        dist = np.linalg.norm(potential_source - current_pos)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            source_idx = idx\n",
    "    \n",
    "    return source_idx\n",
    "\n",
    "def compute_correlation_within_mask(mask, param1, param2):\n",
    "    # Flatten the arrays\n",
    "    mask_flat = mask.flatten()\n",
    "    param1_flat = param1.flatten()\n",
    "    param2_flat = param2.flatten()\n",
    "\n",
    "    param1_flat[np.isnan(param1_flat)] = 0\n",
    "    param2_flat[np.isnan(param2_flat)] = 0\n",
    "\n",
    "    # Ensure the mask has the same shape as the parameters\n",
    "    if mask_flat.shape != param1_flat.shape or mask_flat.shape != param2_flat.shape:\n",
    "        raise ValueError(\"Mask and parameters must have the same shape\")\n",
    "\n",
    "    # Apply the mask\n",
    "    param1_masked = param1_flat[mask_flat > 0]\n",
    "    param2_masked = param2_flat[mask_flat > 0]\n",
    "\n",
    "    # Check if there are enough valid pixels\n",
    "    if len(param1_masked) < 2 or len(param2_masked) < 2:\n",
    "        # print(f\"Not enough valid pixels: len(param1_masked)={len(param1_masked)}, len(param2_masked)={len(param2_masked)}\")\n",
    "        return np.nan  # or return 0.0 or some other value indicating no correlation\n",
    "    \n",
    "    # Compute and return the Pearson correlation\n",
    "    return pearsonr(param1_masked, param2_masked)[0]\n",
    "\n",
    "def get_correlations_for_mask(mask,xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure):\n",
    "    xch4_correlation = compute_correlation_within_mask(mask, xch4, xch4)\n",
    "    swir_surface_albedo_correlation = compute_correlation_within_mask(mask, xch4, swir_surface_albedo)\n",
    "    aerosol_optical_thickness_correlation = compute_correlation_within_mask(mask, xch4, aerosol_optical_thickness)\n",
    "    chi_squared_correlation = compute_correlation_within_mask(mask, xch4, chi_squared)\n",
    "    surface_pressure_correlation = compute_correlation_within_mask(mask, xch4, surface_pressure)\n",
    "\n",
    "    correlations = {\n",
    "        'xch4': xch4_correlation,\n",
    "        'swir_surface_albedo': swir_surface_albedo_correlation,\n",
    "        'aerosol_optical_thickness': aerosol_optical_thickness_correlation,\n",
    "        'chi_squared': chi_squared_correlation,\n",
    "        'surface_pressure': surface_pressure_correlation\n",
    "    }\n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def custom_dilation(starting_point, threshold, plume_mask, max_depth):\n",
    "    image_shape = plume_mask.shape\n",
    "    seed_mask = np.zeros(image_shape, dtype=bool)\n",
    "    visited = np.zeros(image_shape, dtype=bool)  # To keep track of visited pixels\n",
    "    seed_mask[starting_point] = True\n",
    "    visited[starting_point] = True\n",
    "    \n",
    "    # Each element in the queue stores the pixel and the current depth\n",
    "    queue = deque([(starting_point, 0)])  # Start with depth 0\n",
    "    \n",
    "    # Directions for 8-connectivity (N, S, E, W, NE, NW, SE, SW)\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "\n",
    "    while queue:\n",
    "        current_pixel, current_depth = queue.popleft()\n",
    "        \n",
    "        # Stop processing if maximum depth is reached\n",
    "        if current_depth >= max_depth:\n",
    "            continue\n",
    "        \n",
    "        for d in directions:\n",
    "            neighbor = (current_pixel[0] + d[0], current_pixel[1] + d[1])\n",
    "            # Check boundaries\n",
    "            if 0 <= neighbor[0] < image_shape[0] and 0 <= neighbor[1] < image_shape[1]:\n",
    "                # Check if neighbor meets threshold and hasn't been visited\n",
    "                if plume_mask[neighbor] >= threshold and not visited[neighbor]:\n",
    "                    visited[neighbor] = True\n",
    "                    seed_mask[neighbor] = True\n",
    "                    # Add the neighbor with incremented depth\n",
    "                    queue.append((neighbor, current_depth + 1))\n",
    "\n",
    "    return seed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import binary_dilation, generate_binary_structure\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.ndimage import binary_erosion\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "# Set the number of plumes you want to process\n",
    "num_plumes_to_process = 50\n",
    "\n",
    "# Get indices of all plumes\n",
    "plume_indices = [i for i in range(len(predictions_2020)) if predictions_2020[i] > 0.5]\n",
    "\n",
    "# Randomly sample indices from the plume_indices list\n",
    "random_plume_indices = random.sample(plume_indices, num_plumes_to_process)\n",
    "\n",
    "with h5py.File('2020/all_scenes_combined.h5', 'r') as hf:\n",
    "    for i in plume_indices:\n",
    "        # Process each image in the batch\n",
    "        input_image = hf['all_scenes'][i:i+1, :, :, :]\n",
    "        input_image = np.transpose(input_image, (0, 2, 3, 1))\n",
    "\n",
    "        cam, predicted_label = get_heatmap(cam_model, input_image[:, :, :, channel_map['normalized_ch4']])\n",
    "        xch4 = input_image[0, :, :, channel_map['xch4_corrected']]\n",
    "        xch4_non_corrected = input_image[0, :, :, channel_map['xch4']]\n",
    "        norm_xch4 = input_image[0, :, :, channel_map['normalized_ch4']]\n",
    "        u10 = input_image[0, :, :, channel_map['u10']][:, :, 0]\n",
    "        v10 = input_image[0, :, :, channel_map['v10']][:, :, 0]\n",
    "        swir_surface_albedo = input_image[0, :, :, channel_map['surface_albedo']][:, :, 1]\n",
    "        aerosol_optical_thickness = input_image[0, :, :, channel_map['aerosol_optical_thickness']][:, :, 1]\n",
    "        chi_squared = input_image[0, :, :, channel_map['chi_squared']][:, :, 0]\n",
    "        surface_pressure = input_image[0, :, :, channel_map['surface_pressure']]\n",
    "\n",
    "\n",
    "        # Calculate methane enhancement\n",
    "        thresh = (np.nanmean(xch4) - np.nanstd(xch4))\n",
    "        test = xch4.copy()\n",
    "\n",
    "        rel = xch4.copy() - thresh\n",
    "        #generating plume mask\n",
    "        test[test < thresh] = 0\n",
    "        test[test> thresh] = 1\n",
    "        plume_mask = test[:,:,0].astype(np.uint8)*cam\n",
    "\n",
    "\n",
    "        # Define starting point\n",
    "        starting_point = np.unravel_index(np.argmax(plume_mask), plume_mask.shape)\n",
    "\n",
    "        # Define a structural element that includes diagonal connectivity\n",
    "        structure = generate_binary_structure(2, 2)  # 2D connectivity, 2 for including diagonals\n",
    "\n",
    "        # High Threshold mask\n",
    "\n",
    "        high_conf_threshold = np.mean(plume_mask) + (np.std(plume_mask) * 3)\n",
    "        high_conf_mask_ = (plume_mask > high_conf_threshold).astype(np.uint8)\n",
    "\n",
    "    \n",
    "        # Low Threshold mask\n",
    "        low_conf_threshold = np.mean(plume_mask) + np.std(plume_mask) * 0.8\n",
    "        low_conf_mask_ = (plume_mask >= low_conf_threshold).astype(np.uint8)\n",
    "\n",
    "        \n",
    "        high_conf_mask = custom_dilation(starting_point, high_conf_threshold, plume_mask, 20)\n",
    "        low_conf_mask = custom_dilation(starting_point, low_conf_threshold, plume_mask, 10)\n",
    "        low_conf_mask_2 = custom_dilation(starting_point, low_conf_threshold, plume_mask, 10)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        If an enchancement in the xch4 field is caused by a surface(albedo) feature \n",
    "        or by (enchanced) scattering in the atomsphere, which is represented by the\n",
    "        aerosol optical thickness, then we expect their a spatial patterns to be \n",
    "        similar. Therefore, we calculate the correlation between the xch4 field and\n",
    "        the surface albedo(SWIR), aerosol optical thickness, chi squared\n",
    "        (an indicator) for retrieval fit quality, and the surface pressure across the\n",
    "        plume mask. We calculate these correlations for both the high and low confidence plume\n",
    "        masks, 1- and 2-times dilated versions of the low confidence mask, and the entire image. \n",
    "        \"\"\"\n",
    "\n",
    "        high_conf_mask_correlations = get_correlations_for_mask(high_conf_mask, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure)\n",
    "\n",
    "        low_conf_mask_correlations = get_correlations_for_mask(low_conf_mask, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure)\n",
    "\n",
    "        low_conf_mask_2_correlations = get_correlations_for_mask(low_conf_mask_2, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure)\n",
    "\n",
    "        all_mask_correlations = get_correlations_for_mask(np.ones_like(xch4), xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure)\n",
    "\n",
    "        \"\"\"\n",
    "        Another major indicator for artificats is a mismatch between the direction of the plume and\n",
    "        the direction of the wind field. By applying a principal component analysis to the plume mask, we compute the two main axes of the pixels in the high confidence plume mask, after re-projecting the pixel centers to meter space and weighting them by there enchancement relative to the background. We use the ratio of the explained variance of the two axes as a measure of the elongation of the plume. We then compute the angle between the main axis of the plume and the wind direction(averaged across the plume mask) the smaller the difference, the more likely the plume is real. We also use the wind field to estimate the source of the plume by taking the most upwind pixel in the high confidence plume mask and moving it in the opposite direction of the wind field. The pixel with the shortest distance to the plume mask is considered the source of the plume.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute plume shape and wind direction alignment\n",
    "        elongation_ratio, direction_difference = compute_plume_shape_and_direction(high_conf_mask, u10, v10)\n",
    "\n",
    "        # Find the plume source\n",
    "        source_idx = find_plume_source(high_conf_mask, u10, v10)\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "        \n",
    "        if source_idx is None:\n",
    "            pass\n",
    "        else:\n",
    "            if high_conf_mask_correlations['swir_surface_albedo'] < 0.9 and high_conf_mask_correlations['aerosol_optical_thickness'] < 0.9 and high_conf_mask_correlations['chi_squared'] < 0.9 and high_conf_mask_correlations['surface_pressure'] < 0.9 and direction_difference < 1.7:\n",
    "                print(high_conf_threshold)\n",
    "                print(f\"elongation ratio: {elongation_ratio}\\ndirection difference:{direction_difference}\\n\\nhigh_conf_corelations: {high_conf_mask_correlations}\")#\\n\\nlow_conf_corelations: {low_conf_mask_correlations}\\n\\nlow_conf_2_corelations:  #{low_conf_mask_2_correlations}\\n\\nall_corelations: {all_mask_correlations}\")\n",
    "\n",
    "                subplots = 3\n",
    "                fig, ax = plt.subplots(1, subplots, figsize=(5*subplots, 5),dpi = 80)\n",
    "                im0 = ax[0].imshow(xch4, cmap='rainbow')\n",
    "                #overlay cam on xch4\n",
    "                ax[0].imshow(cam, cmap='grey', alpha=0.6)\n",
    "                ax[0].invert_yaxis()\n",
    "                ax[0].set_xticks(np.arange(0, 32, 5))\n",
    "\n",
    "\n",
    "                cbar0 = plt.colorbar(im0, ax=ax[0], location='right', shrink = 0.7, pad=0.06)  \n",
    "                cbar0.ax.set_title('[ppb]', loc='center', fontsize=12)\n",
    "\n",
    "                im1 = ax[1].imshow(cam, cmap='viridis')\n",
    "                ax[1].invert_yaxis()\n",
    "                ax[1].axis('off')  # Hides the axis\n",
    "                cbar1 = plt.colorbar(im1, ax=ax[1], location='right',shrink = 0.7, pad=0.06)  #\n",
    "                cbar1.set_ticks(np.arange(0,1,0.2))\n",
    "        \n",
    "                # xch4 relative to the background\n",
    "                im2 = ax[2].imshow(rel, cmap='rainbow')\n",
    "                \n",
    "                ax[2].imshow(high_conf_mask, cmap='grey', alpha=0.6)\n",
    "                ax[2].plot(source_idx[1], source_idx[0], 'kx', markersize=10)\n",
    "                ax[2].invert_yaxis()\n",
    "                cbar2 = plt.colorbar(im2, ax=ax[2], location='right', shrink = 0.7, pad=0.06)\n",
    "                cbar2.ax.set_title('[ppb]', loc='center', fontsize=12)\n",
    "                ax[2].set_xticks(np.arange(0, 32, 5))\n",
    "                plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 file\n",
    "with h5py.File('2020/all_scenes_combined.h5', 'r') as hf:\n",
    "    all_scenes = hf['all_scenes']  # Access the dataset directly\n",
    "    \n",
    "    # Initialize a list to store selected scenes\n",
    "    selected_scenes = []\n",
    "\n",
    "    # Iterate over the dataset and process in chunks\n",
    "    for i, prediction in enumerate(predictions_2020):\n",
    "        if prediction > 0.5:\n",
    "            selected_scenes.append(all_scenes[i])  # Append the relevant scene\n",
    "\n",
    "    # Convert the list to a NumPy array\n",
    "    selected_scenes = np.array(selected_scenes)\n",
    "    \n",
    "    # Save the filtered scenes to a new HDF5 file or .npy file\n",
    "    with h5py.File('filtered_scenes5.h5', 'w') as hf_filtered:\n",
    "        hf_filtered.create_dataset('plume_scenes', data=selected_scenes)\n",
    "    \n",
    "    # Optionally, you can also save as a .npy file\n",
    "    # np.save('plume_scenes.npy', selected_scenes)\n",
    "    \n",
    "    # Print the filtered scenes to confirm\n",
    "    print(\"Filtered Scenes:\", selected_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import generate_binary_structure\n",
    "\n",
    "# Assuming you have all the necessary functions like get_heatmap, custom_dilation, get_correlations_for_mask, compute_plume_shape_and_direction, find_plume_source defined\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('2020/2020_plumes.h5', 'r') as hf:\n",
    "    plume_scenes = hf['plume_scenes']\n",
    "    \n",
    "    # Iterate over the first 10 scenes (adjust as needed)\n",
    "    for i in range(50):\n",
    "        # Access a batch (in this case, just one image at a time)\n",
    "        input_image = plume_scenes[i:i+1, :, :, :]\n",
    "        input_image = np.transpose(input_image, (0, 2, 3, 1))\n",
    "\n",
    "        # Process the image as you outlined\n",
    "        cam, predicted_label = get_heatmap(cam_model, input_image[:, :, :, channel_map['normalized_ch4']])\n",
    "        xch4 = input_image[0, :, :, channel_map['xch4_corrected']]\n",
    "        xch4_non_corrected = input_image[0, :, :, channel_map['xch4']]\n",
    "        norm_xch4 = input_image[0, :, :, channel_map['normalized_ch4']]\n",
    "        u10 = input_image[0, :, :, channel_map['u10']][:, :, 0]\n",
    "        v10 = input_image[0, :, :, channel_map['v10']][:, :, 0]\n",
    "        swir_surface_albedo = input_image[0, :, :, channel_map['surface_albedo']][:, :, 1]\n",
    "        aerosol_optical_thickness = input_image[0, :, :, channel_map['aerosol_optical_thickness']][:, :, 1]\n",
    "        chi_squared = input_image[0, :, :, channel_map['chi_squared']][:, :, 0]\n",
    "        surface_pressure = input_image[0, :, :, channel_map['surface_pressure']]\n",
    "\n",
    "        # Calculate methane enhancement\n",
    "        thresh = np.nanmean(xch4) - np.nanstd(xch4)\n",
    "        test = xch4.copy()\n",
    "\n",
    "        rel = xch4.copy() - thresh\n",
    "        test[test < thresh] = 0\n",
    "        test[test > thresh] = 1\n",
    "        plume_mask = test[:, :, 0].astype(np.uint8) * cam\n",
    "\n",
    "        # Define starting point for mask generation\n",
    "        starting_point = np.unravel_index(np.argmax(plume_mask), plume_mask.shape)\n",
    "\n",
    "        # Define a structural element that includes diagonal connectivity\n",
    "        structure = generate_binary_structure(2, 2)  # 2D connectivity, including diagonals\n",
    "\n",
    "        # Generate high and low confidence masks\n",
    "        high_conf_threshold = np.mean(plume_mask) + (np.std(plume_mask) * 3)\n",
    "        low_conf_threshold = np.mean(plume_mask) + np.std(plume_mask) * 0.8\n",
    "\n",
    "        high_conf_mask = custom_dilation(starting_point, high_conf_threshold, plume_mask, 20)\n",
    "        low_conf_mask = custom_dilation(starting_point, low_conf_threshold, plume_mask, 10)\n",
    "        low_conf_mask_2 = custom_dilation(starting_point, low_conf_threshold, plume_mask, 10)\n",
    "\n",
    "        # Calculate correlations for different masks\n",
    "        high_conf_mask_correlations = get_correlations_for_mask(\n",
    "            high_conf_mask, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure\n",
    "        )\n",
    "        low_conf_mask_correlations = get_correlations_for_mask(\n",
    "            low_conf_mask, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure\n",
    "        )\n",
    "        low_conf_mask_2_correlations = get_correlations_for_mask(\n",
    "            low_conf_mask_2, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure\n",
    "        )\n",
    "        all_mask_correlations = get_correlations_for_mask(\n",
    "            np.ones_like(xch4), xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure\n",
    "        )\n",
    "\n",
    "        # Compute plume shape and wind direction alignment\n",
    "        elongation_ratio, direction_difference = compute_plume_shape_and_direction(high_conf_mask, u10, v10)\n",
    "\n",
    "        # Find the plume source\n",
    "        source_idx = find_plume_source(high_conf_mask, u10, v10)\n",
    "\n",
    "        # Conditional processing and visualization\n",
    "\n",
    "\n",
    "        if source_idx is None:\n",
    "            pass\n",
    "        else:\n",
    "            if (high_conf_mask_correlations['swir_surface_albedo'] < 0.9 and\n",
    "                high_conf_mask_correlations['aerosol_optical_thickness'] < 0.9 and\n",
    "                high_conf_mask_correlations['chi_squared'] < 0.9 and\n",
    "                high_conf_mask_correlations['surface_pressure'] < 0.9 and\n",
    "                direction_difference < 1.7):\n",
    "\n",
    "                print(f\"High confidence threshold: {high_conf_threshold}\")\n",
    "                print(f\"Elongation ratio: {elongation_ratio}\\nDirection difference: {direction_difference}\")\n",
    "                print(f\"High confidence correlations: {high_conf_mask_correlations}\")\n",
    "\n",
    "                # Visualization of results\n",
    "                subplots = 3\n",
    "                fig, ax = plt.subplots(1, subplots, figsize=(5*subplots, 5), dpi=80)\n",
    "                \n",
    "                im0 = ax[0].imshow(xch4, cmap='rainbow')\n",
    "                ax[0].invert_yaxis()\n",
    "                ax[0].set_xticks(np.arange(0, 32, 5))\n",
    "                cbar0 = plt.colorbar(im0, ax=ax[0], location='right', shrink=0.7, pad=0.06)  \n",
    "                cbar0.ax.set_title('[ppb]', loc='center', fontsize=12)\n",
    "\n",
    "                im1 = ax[1].imshow(cam, cmap='viridis')\n",
    "                ax[1].invert_yaxis()\n",
    "                ax[1].axis('off')\n",
    "                cbar1 = plt.colorbar(im1, ax=ax[1], location='right', shrink=0.7, pad=0.06)\n",
    "                cbar1.set_ticks(np.arange(0, 1, 0.2))\n",
    "        \n",
    "                # xch4 relative to the background\n",
    "                im2 = ax[2].imshow(rel, cmap='rainbow')\n",
    "                ax[2].imshow(high_conf_mask, cmap='grey', alpha=0.6)\n",
    "                ax[2].plot(source_idx[1], source_idx[0], 'kx', markersize=10)\n",
    "                ax[2].invert_yaxis()\n",
    "                cbar2 = plt.colorbar(im2, ax=ax[2], location='right', shrink=0.7, pad=0.06)\n",
    "                cbar2.ax.set_title('[ppb]', loc='center', fontsize=12)\n",
    "                ax[2].set_xticks(np.arange(0, 32, 5))\n",
    "\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 scene seperated data (met filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the existing HDF5 file\n",
    "with h5py.File('2020/selected_plume_scenes2.h5', 'r') as hf:\n",
    "    # Read the plume_scenes dataset\n",
    "    plume_scenes = hf['plume_scenes'][:]\n",
    "\n",
    "# Remove the unnecessary dimension using np.squeeze\n",
    "# This will remove any singleton dimensions, particularly the second dimension in your case\n",
    "squeezed_scenes = np.squeeze(plume_scenes, axis=1)\n",
    "\n",
    "# Save the modified data back to a new HDF5 file\n",
    "with h5py.File('2020/selected_plume_scenes_squeezed.h5', 'w') as hf_new:\n",
    "    hf_new.create_dataset('plume_scenes', data=squeezed_scenes)\n",
    "\n",
    "print(f\"Saved squeezed data with shape {squeezed_scenes.shape} to '2020/selected_plume_scenes_squeezed.h5'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import generate_binary_structure\n",
    "\n",
    "# Assuming you have all the necessary functions like get_heatmap, custom_dilation, get_correlations_for_mask, compute_plume_shape_and_direction, find_plume_source defined\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('2020/selected_plume_scenes_squeezed.h5', 'r') as hf:\n",
    "    plume_scenes = hf['plume_scenes']\n",
    "    print(plume_scenes.shape)\n",
    "\n",
    "    # Iterate over the first 50 scenes (adjust as needed)\n",
    "    for i in range(0,50):\n",
    "        # Access a batch (in this case, just one image at a time)\n",
    "        input_image = plume_scenes[i:i+1, :, :, :]\n",
    "\n",
    "        # Process the image as you outlined\n",
    "        cam, predicted_label = get_heatmap(cam_model, input_image[:, :, :, channel_map['normalized_ch4']])\n",
    "        xch4 = input_image[0, :, :, channel_map['xch4_corrected']]\n",
    "        xch4_non_corrected = input_image[0, :, :, channel_map['xch4']]\n",
    "        norm_xch4 = input_image[0, :, :, channel_map['normalized_ch4']]\n",
    "        u10 = input_image[0, :, :, channel_map['u10']][:, :, 0]\n",
    "        v10 = input_image[0, :, :, channel_map['v10']][:, :, 0]\n",
    "        swir_surface_albedo = input_image[0, :, :, channel_map['surface_albedo']][:, :, 1]\n",
    "        aerosol_optical_thickness = input_image[0, :, :, channel_map['aerosol_optical_thickness']][:, :, 1]\n",
    "        chi_squared = input_image[0, :, :, channel_map['chi_squared']][:, :, 0]\n",
    "        surface_pressure = input_image[0, :, :, channel_map['surface_pressure']]\n",
    "\n",
    "        # Calculate methane enhancement\n",
    "        thresh = (np.nanmean(xch4) - np.nanstd(xch4))\n",
    "        test = xch4.copy()\n",
    "\n",
    "        rel = xch4.copy() - thresh\n",
    "        #generating plume mask\n",
    "        test[test < thresh] = 0\n",
    "        test[test > thresh] = 1\n",
    "        plume_mask = test[:, :, 0].astype(np.uint8) * cam\n",
    "\n",
    "        # Determine starting point based on the hottest pixel in the CAM output\n",
    "        starting_point = np.unravel_index(np.argmax(cam), cam.shape)\n",
    "\n",
    "        # Define a structural element that includes diagonal connectivity\n",
    "        structure = generate_binary_structure(2, 2)  # 2D connectivity, including diagonals\n",
    "\n",
    "        # High Threshold mask\n",
    "        high_conf_threshold = np.mean(plume_mask) + (np.std(plume_mask) * 1.8)\n",
    "        high_conf_mask_ = (plume_mask > high_conf_threshold).astype(np.uint8)\n",
    "\n",
    "        # Low Threshold mask\n",
    "        low_conf_threshold = np.mean(plume_mask) + np.std(plume_mask) * 0.8\n",
    "        low_conf_mask_ = (plume_mask >= low_conf_threshold).astype(np.uint8)\n",
    "\n",
    "        high_conf_mask = custom_dilation(starting_point, high_conf_threshold, high_conf_mask_, 10)\n",
    "        low_conf_mask = custom_dilation(starting_point, low_conf_threshold, plume_mask, 10)\n",
    "        low_conf_mask_2 = custom_dilation(starting_point, low_conf_threshold, plume_mask, 10)\n",
    "\n",
    "        # Calculate correlations for different masks\n",
    "        high_conf_mask_correlations = get_correlations_for_mask(\n",
    "            high_conf_mask, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure\n",
    "        )\n",
    "        low_conf_mask_correlations = get_correlations_for_mask(\n",
    "            low_conf_mask, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure\n",
    "        )\n",
    "        low_conf_mask_2_correlations = get_correlations_for_mask(\n",
    "            low_conf_mask_2, xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure\n",
    "        )\n",
    "        all_mask_correlations = get_correlations_for_mask(\n",
    "            np.ones_like(xch4), xch4, swir_surface_albedo, aerosol_optical_thickness, chi_squared, surface_pressure\n",
    "        )\n",
    "\n",
    "        # Compute plume shape and wind direction alignment\n",
    "        elongation_ratio, direction_difference = compute_plume_shape_and_direction(high_conf_mask, u10, v10)\n",
    "\n",
    "        # Find the plume source\n",
    "        source_idx = find_plume_source(high_conf_mask, u10, v10)\n",
    "\n",
    "        # Conditional processing and visualization\n",
    "        if source_idx is not None:\n",
    "            if (high_conf_mask_correlations['swir_surface_albedo'] < 0.9 and\n",
    "                high_conf_mask_correlations['aerosol_optical_thickness'] < 0.9 and\n",
    "                high_conf_mask_correlations['chi_squared'] < 0.9 and\n",
    "                high_conf_mask_correlations['surface_pressure'] < 0.9 and\n",
    "                direction_difference < 1.7):\n",
    "\n",
    "                print(f\"High confidence threshold: {high_conf_threshold}\")\n",
    "                print(f\"Elongation ratio: {elongation_ratio}\\nDirection difference: {direction_difference}\")\n",
    "                print(f\"High confidence correlations: {high_conf_mask_correlations}\")\n",
    "\n",
    "                # Visualization of results (optional)\n",
    "                subplots = 3\n",
    "                fig, ax = plt.subplots(1, subplots, figsize=(5*subplots, 5), dpi=80)\n",
    "                \n",
    "                im0 = ax[0].imshow(xch4, cmap='rainbow')\n",
    "                ax[0].invert_yaxis()\n",
    "                ax[0].set_xticks(np.arange(0, 32, 5))\n",
    "                cbar0 = plt.colorbar(im0, ax=ax[0], location='right', shrink=0.7, pad=0.06)  \n",
    "                cbar0.ax.set_title('[ppb]', loc='center', fontsize=12)\n",
    "\n",
    "                im1 = ax[1].imshow(cam, cmap='viridis')\n",
    "                ax[1].invert_yaxis()\n",
    "                ax[1].axis('off')\n",
    "                cbar1 = plt.colorbar(im1, ax=ax[1], location='right', shrink=0.7, pad=0.06)\n",
    "                cbar1.set_ticks(np.arange(0, 1, 0.2))\n",
    "        \n",
    "                # xch4 relative to the background\n",
    "                im2 = ax[2].imshow(rel, cmap='rainbow')\n",
    "                ax[2].imshow(high_conf_mask, cmap='grey', alpha=0.6)\n",
    "                ax[2].plot(source_idx[1], source_idx[0], 'kx', markersize=10)\n",
    "                # ax[2].plot(starting_point[1], starting_point[0], 'ro', markersize=10)\n",
    "                ax[2].invert_yaxis()\n",
    "                cbar2 = plt.colorbar(im2, ax=ax[2], location='right', shrink=0.7, pad=0.06)\n",
    "                cbar2.ax.set_title('[ppb]', loc='center', fontsize=12)\n",
    "                ax[2].set_xticks(np.arange(0, 32, 5))\n",
    "\n",
    "                plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
